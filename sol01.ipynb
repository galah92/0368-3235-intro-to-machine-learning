{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "- Let $A = XX^T \\in \\mathbb{M}_{n \\times n}^{\\mathbb{R}}$ be a symetric matrix, then $$\\forall v\\in \\mathbb{R}^n. v^{T}Av = v^{T}XX^{T}v = (v^TX)(X^{T}v) = (v^TX)^T(v^TX) \\ge 0$$Where the last inequality holds because we're taking the dot product of $v^TX$.\n",
    "- Let $A \\in \\mathbb{M}_{n \\times n}^{\\mathbb{R}}$ be a PSD symatric matrix. Recall that by diagnolization $A = Q^TDQ$, where $Q$ is orthogonal matrix and $D = diag(\\lambda_1, \\ldots , \\lambda_n)$.  \n",
    "Moreover, $\\forall i\\in \\{0,\\ldots ,n\\}. \\lambda_i \\ge 0$ because $v^T\\lambda_i v = v^TAv \\ge 0$ (where $v$ is the eigenvector of $\\lambda_i$).  \n",
    "Denote $E = diag(\\sqrt{\\lambda_1}, \\ldots, \\sqrt{\\lambda_n})$, and define $X = Q^TEQ$. Thus,\n",
    "$$XX^T = (Q^TEQ)(Q^TEQ)^T = Q^TEE^TQ = QDQ = A$$\n",
    "Second equality is because $Q$ is orthogonal, third is by definition of $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "Let $A, B \\in \\mathbb{M}_{n \\times n}^{\\mathbb{R}}$ be a symetric PSD matrices, thus\n",
    "$$\\forall \\theta\\in [0,1], v \\in \\mathbb{R}^n. v^T ( \\theta A + (1 - \\theta) B) v\n",
    "= \\theta ( v^T A v) + (1 - \\theta) v^T B v\n",
    "\\ge 0$$Because each of the terms is PSD, $\\theta \\ge 0$ and $(1 - \\theta) \\ge 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus and Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "$$\n",
    "\\begin{align*}\n",
    "(\\frac{\\partial x^T A x}{\\partial x})_k\n",
    "& = \\frac{\\partial x^T A x}{\\partial x_k} \\\\\n",
    "& = \\frac{\\partial}{\\partial x_k} \\sum_i \\sum_j A_{ij} x_i x_j \\\\\n",
    "& = \\frac{\\partial}{\\partial x_k} [ \\sum_k A_{kk} x_k^2 + \\sum_{i\\neq k} A_{ik} x_i x_k + \\sum_{j\\neq k} A_{kj} x_j x_k ] \\\\\n",
    "& = 2\\sum_k A_{kk} x_k + \\sum_{i\\neq k} A_{ik} x_i + \\sum_{j\\neq k} A_{kj} x_j \\\\\n",
    "& = \\sum_i A_ik x_i + \\sum_j A_kj x_j\n",
    "\\end{align*}\n",
    "$$\n",
    "And thus $$\\frac{\\partial x^T A x}{x} = A x + A^T x = (A + A^T) x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "Given the problem\n",
    "$$\\max{H(\\mathbf{p}) = -\\sum_{i=1}^n p_i\\cdot \\log_2{p_i}} \\quad \\text{s.t} \\sum_{i=1}^n p_i = 1$$\n",
    "The lagrangian is\n",
    "$$\\mathcal{L}(\\mathbf{p}, \\lambda) = -\\sum_{i=1}^n p_i\\cdot \\log_2{p_i} - \\lambda (\\sum_{i=1}^n p_i - 1)$$\n",
    "We now take the partial derivatives and set them to zero:\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial p_i}\n",
    "= - (\\log_2{p_i} + \\frac{p_i}{p_i}) + \\lambda\n",
    "= - \\log_2{p_i} - 1 + \\lambda = 0\n",
    "\\implies p_i = 2^{\\lambda + 1} \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial \\lambda}\n",
    "= 1 - \\sum_{i=1}^n p_i = 0\n",
    "\\implies 1 = \\sum_{i=1}^n p_i = \\sum_{i=1}^n 2^{\\lambda + 1} = 2n \\cdot 2^{\\lambda} \\\\\n",
    "& \\implies \\lambda = \\log_2{\\frac{1}{2n}} = - \\log_2{2n} = 1 - \\log_2{n} \\\\\n",
    "& \\implies p_i = 2^{1 - \\log_2{n}} + 1 = 2^{- \\log_2{n}} = \\frac{1}{n}\n",
    "\\end{align*}\n",
    "$$\n",
    "And we got the uniform distribution, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{P}[X_0 \\ge max(X_1, \\ldots, X_{n-1})]\n",
    "& = \\int_0^{\\infty} \\mathbb{P}[X_0 \\ge max(X_1, \\ldots, X_{n-1}) | X_0 = a] \\, f_X(a) \\, da \\tag{marginal probabilty} \\\\\n",
    "& = \\int_0^{\\infty} \\mathbb{P}[max(X_1, \\ldots, X_{n-1}) \\le a] \\, f_X(a) \\, da \\\\\n",
    "& = \\int_0^{\\infty} \\prod_{i=1}^{n}({\\mathbb{P}[X_i \\le a]}) \\, f_X(a) \\, da \\tag{independency} \\\\\n",
    "& = \\int_0^{\\infty} F_X^{n-1}(a) \\, f_X(a) \\, da \\tag{CDF definition} \\\\\n",
    "& = \\lim_{a \\to \\infty} \\frac{1}{n}F_X^n(a) - \\frac{1}{n}F_X^n(0) \\tag{fund' theorem of calculus} \\\\\n",
    "& = \\frac{1}{n}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Rules and Concetration Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "$$\n",
    "\\begin{align*}\n",
    "& P[Y=1|X=x] > P[Y=0|X=x] \\\\\n",
    "& \\implies \\frac{P[X=x|Y=1]P[Y=1]}{P[X=x]} > \\frac{P[X=x|Y=0]P[Y=0]}{P[X=x]} \\tag{Bayes rule} \\\\\n",
    "& \\implies \\frac{P[X=x|Y=1]}{P[X=x|Y=0]} > \\frac{P[Y=0]}{P[Y=0]} \\\\\n",
    "& \\implies \\frac{\\exp{[-\\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1)]}}{\\exp{[-\\frac{1}{2}(x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0)]}} > \\frac{1-p}{p} \\tag{Multivariate Gaussian} \\\\\n",
    "& \\implies -\\frac{1}{2}\\Sigma^{-1}[(x-\\mu_1)^T(x-\\mu_1)-(x-\\mu_0)^T(x-\\mu_0)] > \\ln{\\frac{1-p}{p}} \\\\\n",
    "& \\implies {\\lVert x-\\mu_1 \\rVert}_2^2 - {\\lVert x-\\mu_0 \\rVert}_2^2 > -2\\Sigma \\cdot \\ln{\\frac{1-p}{p}} \\\\\n",
    "& \\implies {\\lVert x \\rVert}^2 + {\\lVert \\mu_1 \\rVert}^2 - 2x^T\\mu_1 - {\\lVert x \\rVert}^2 - {\\lVert \\mu_0 \\rVert}^2 + 2x^T\\mu_0 > -2\\Sigma \\cdot \\ln{\\frac{1-p}{p}} \\\\\n",
    "& \\implies {\\lVert \\mu_1 \\rVert}^2 - {\\lVert \\mu_0 \\rVert}^2 + 2x^T(\\mu_0 - \\mu_1) > -2\\Sigma \\cdot \\ln{\\frac{1-p}{p}} \\\\\n",
    "& \\implies x^T > \\frac{2\\Sigma \\cdot \\ln{\\frac{1-p}{p}} + {\\lVert \\mu_1 \\rVert}^2 - {\\lVert \\mu_0 \\rVert}^2}{2(\\mu_1 - \\mu_0)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "The general case of Hoeffding bound for random variables $X_1,\\ldots,X_n$ bounded by the intervals $[a_i, b_i]$ is\n",
    "$$\\mathbb{P}[S-E[S] > t] < \\exp{(-\\frac{2t^2}{\\sum_{i=1}^n{(b_i - a_i)^2}})}$$\n",
    "In our case, $E[S] = \\sum_{i=1}^nE[X_i] = nE[X_1] = n(\\frac{5-3}{2}) = n$, thus\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\mathbb{P}[S > n^2 + 0.2n] = \\mathbb{P}[S - n > n^2 - 0.8n] \\implies t = n^2 - 0.8n \\\\\n",
    "& \\exp{(-\\frac{2t^2}{\\sum_{i=1}^n{(b_i - a_i)^2}})}\n",
    "= \\exp{(-\\frac{2(n^2-0.8n)^2}{n8^2})}\n",
    "= \\exp{(-\\frac{n(n-0.8)^2}{32})}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "###  \n",
    "The probability that server $i$ got job $j$ is $\\frac{1}{m}$, thus\n",
    "$$\n",
    "E[R_i] = \\sum_{j=1}^n{E[R_{ij}]} = \\sum_{j=1}^n{\\frac{1}{m}L_j + \\frac{1}{m}0} = \\sum_{j=1}^n{\\frac{L_j}{m}} = \\frac{L}{m}\n",
    "$$\n",
    "\n",
    "###  \n",
    "$$\n",
    "P[R_i \\ge (1+\\delta)\\cdot E[R_i]]\n",
    "\\le (\\frac{e^{\\delta}}{(1+\\delta)^{(1+\\delta)}})^{\\mu}\n",
    "= (\\frac{e^{0.1}}{1.1^{1.1}})^{\\frac{L}{m}}\n",
    "\\approx 0.995^{\\frac{L}{m}}\n",
    "$$\n",
    "\n",
    "###  \n",
    "Denote $A_i = R_i \\ge (1+\\delta)\\cdot E[R_i]$, then\n",
    "$$\n",
    "\\begin{align*}\n",
    "P[A_1 \\cup \\ldots \\cup A_n]\n",
    "& \\le \\sum_{i=1}^n P[A_i] \\tag{Union bound} \\\\\n",
    "& = n \\cdot P[A_i] \\tag{Identical} \\\\\n",
    "& \\le n\\cdot 0.995^{\\frac{L}{m}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "data = mnist['data']\n",
    "labels = mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "idx = numpy.random.RandomState(0).choice(70000, 11000)\n",
    "train = data[idx[:10000], :].astype(int)\n",
    "train_labels = labels[idx[:1000]]\n",
    "test = data[idx[10000:], :].astype(int)\n",
    "train_labels = labels[idx[1000:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(images, labels, image, k):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
